# -*- coding: utf-8 -*-
"""Sentiment_With_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WcrSSUF6kizhxyrjX70hvILBQ87C1EQU
"""

from google.colab import drive
drive.mount('/content/drive')

# utilities
import re  # for regular expressions
import numpy as np
import pandas as pd

# plotting
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# nltk
from nltk.stem import WordNetLemmatizer
# sklearn
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

# Loading and reading data
df = pd.read_csv('/content/drive/MyDrive/ML_DATA/covid_tweets.csv', encoding='latin-1')
df.head()

# Columns/features in data
df.columns

# Length of the dataset
print('length of data is', len(df))

# Shape of data
df. shape

# Data information
df.info()

# Datatypes of all columns
df.dtypes

# Checking for null values
np.sum(df.isnull().any(axis=1))

# Rows and columns in the dataset
print('Count of columns in the data is:  ', len(df.columns))
print('Count of rows in the data is:  ', len(df))

# Check unique target values
df['Sentiment'].unique()

# Check the number of target values
df['Sentiment'].nunique()

# Data Visualization of Target Variables
# Plotting the distribution for dataset.
ax = df.groupby('Sentiment').count().plot(kind='bar', title='Distribution of data',legend=False)
ax.set_xticklabels(['Neutral', 'Positive', 'Extremely Negative', 'Negative',
       'Extremely Positive'], rotation=0)
# Storing data in lists.
OriginalTweet, sentiment = list(df['OriginalTweet']), list(df['Sentiment'])

# Selecting features/variables
data=df[['OriginalTweet','Sentiment']]
data.head()

# Data Visualization of Target Variables
custom_palette = ["blue", "pink", "gray", "yellow", "red"]
sns.countplot(data, x='Sentiment', palette=custom_palette)
#sns.countplot(x = y)

data['Sentiment'].unique()

# converting Sentiment value from object to int
# Create a mapping dictionary
mapping = {'Extremely Negative':1, 'Negative': 2, 'Neutral':3, 'Positive': 4,'Extremely Positive':5}
# Assign integer values based on the mapping
data['sentiment_int'] = data['Sentiment'].map(mapping)
data.head()

data=data[['OriginalTweet','sentiment_int']]
data.head()

# Check unique target values
data['sentiment_int'].unique()

# Check the number of target values
data['sentiment_int'].nunique()

# Data Visualization of Target Variables
custom_palette = ["blue", "pink", "gray", "yellow", "red"]
sns.countplot(data, x='sentiment_int', palette=custom_palette)
#sns.countplot(x = y)

# Binary classification of Sentiment values
data['sentiment_int'] = np.where(data['sentiment_int'] <= 3, 0, 1)
data.head()

#Separating positive and negative tweets
data_pos = data[data['sentiment_int'] == 1]
data_neg = data[data['sentiment_int'] == 0]

# Taking one-fourth of the data so we can run it on our machine easily
data_pos = data_pos.iloc[:int(10000)]
data_neg = data_neg.iloc[:int(10000)]

# Combining positive and negative tweets
dataset = pd.concat([data_pos, data_neg])
dataset

import warnings
pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Making statement text in lowercase
dataset['OriginalTweet']=dataset['OriginalTweet'].str.lower()
dataset['OriginalTweet'].head()

# importing and downloading all stopwords
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
print(stopwords.words('english'))

# Cleaning and removing the above stop words list from the tweet text
stop_words = set(stopwords.words('english'))
def cleaning_stopwords(OriginalTweet):
    return " ".join([word for word in str(OriginalTweet).split() if word not in stop_words])
dataset['OriginalTweet'] = dataset['OriginalTweet'].apply(lambda OriginalTweet: cleaning_stopwords(OriginalTweet))
dataset['OriginalTweet'].head()

# user-defined function to remove unwanted text patterns from the tweets. It takes two arguments, one is the original string of text and
# the other is the pattern of text that we want to remove from the string. The function returns the same input string but without the given
# pattern. This function will be used to remove the pattern ‘@user’ from all the tweets in our data.

def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
    return input_txt

# Removing Twitter Handles (@user)
dataset['OriginalTweet'] = np.vectorize(remove_pattern)(dataset['OriginalTweet'], "@[\w]*")
dataset.head()

#Cleaning and removing punctuations
import string
english_punctuations = string.punctuation
punctuations_list = english_punctuations
def cleaning_punctuations(OriginalTweet):
    translator = str.maketrans('', '', punctuations_list)
    return OriginalTweet.translate(translator)
dataset['OriginalTweet']= dataset['OriginalTweet'].apply(lambda x: cleaning_punctuations(x))
dataset['OriginalTweet'].head()

#  Cleaning and removing repeating characters
def cleaning_repeating_char(OriginalTweet):
    return re.sub(r'(.)1+', r'1', OriginalTweet)
dataset['OriginalTweet'] = dataset['OriginalTweet'].apply(lambda x: cleaning_repeating_char(x))
dataset['OriginalTweet'].tail()

# Cleaning and removing URLs
def cleaning_URLs(dataset):
    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',dataset)
dataset['OriginalTweet'] = dataset['OriginalTweet'].apply(lambda x: cleaning_URLs(x))
dataset['OriginalTweet'].head()

# Cleaning and removing numeric numbers
def cleaning_numbers(dataset):
    return re.sub('[0-9]+', '', dataset)
dataset['OriginalTweet'] = dataset['OriginalTweet'].apply(lambda x: cleaning_numbers(x))
dataset['OriginalTweet'].head()

# Getting tokenization of tweet text
tokenized_tweet = dataset.OriginalTweet.apply(lambda x: x.split())
tokenized_tweet.head()

# Applying stemming
from nltk.stem.porter import *
stemmer = PorterStemmer()
tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
tokenized_tweet.head()

# Applying lemmatizer
import nltk
nltk.download('wordnet')
lm = nltk.WordNetLemmatizer()
def lemmatizer_on_text(data):
    text = [lm.lemmatize(word) for word in data]
    return data
dataset['OriginalTweet'] = dataset['OriginalTweet'].apply(lambda x: lemmatizer_on_text(x))
dataset['OriginalTweet'].head()

# Removing Short Words
dataset.OriginalTweet = dataset.OriginalTweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))
dataset['OriginalTweet'].head()

dataset.head()

# Separating input feature and label
X=data.OriginalTweet
y=data.sentiment_int

# Plot a cloud of words for negative tweets
data_neg = data['OriginalTweet'][:800000]
plt.figure(figsize = (12, 12))
wc = WordCloud(max_words = 1000 , width = 1200 , height = 600,
               collocations=False).generate(" ".join(data_neg))
plt.imshow(wc)

# Plot a cloud of words for positive tweets
data_pos = data['OriginalTweet'][:800000]
plt.figure(figsize = (10, 12))
wc = WordCloud(max_words = 1000 , width = 1200 , height = 600,
            collocations=False).generate(" ".join(data_pos))
plt.imshow(wc)

# Separating the 97% data for training data and 3% for testing data
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.03, random_state =26105111)

print(X.shape)
print(X_train.shape)
print(X_test.shape)

# Fit the TF-IDF Vectorizer
vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=600000)
vectoriser.fit(X_train)
#print('No. of feature_words:', len(vectoriser.get_feature_names()))

# Transform the data using TF-IDF Vectorizer
X_train = vectoriser.transform(X_train)
X_test  = vectoriser.transform(X_test)

# Defining function for model evaluation
def model_Evaluate(model):
# Predict values for Test dataset
  y_pred = model.predict(X_test)
# Print the evaluation metrics for the dataset.
  print(classification_report(y_test, y_pred))

# Model 1 Optional
# Bernoulli Naive Bayes function
BNBmodel = BernoulliNB()
BNBmodel.fit(X_train, y_train)
model_Evaluate(BNBmodel)
y_pred1 = BNBmodel.predict(X_test)

# Model 2
# Fit the LinearSVC
SVCmodel = LinearSVC()
SVCmodel.fit(X_train, y_train)
model_Evaluate(SVCmodel)
y_pred2 = SVCmodel.predict(X_test)

# Model 3
# Train the SVM model
from sklearn import svm
from sklearn import metrics
model = svm.SVC(gamma = 'scale', kernel='linear')
#model = model.SVC(kernel='linear')
# Fit the SVM model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred3 = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred3))

predicted = model.predict(X_test)
accuracy_score = metrics.accuracy_score(predicted, y_test)
print("Accuracuy Score: ",accuracy_score)

# Plotting predicted probabilities for the positive class using predict_proba
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_pred3)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC CURVE')
plt.legend(loc="lower right")
plt.show()

y_pred = SVCmodel.predict(X_test)
cm = confusion_matrix(y_pred,y_test)
cm

ConfusionMatrixDisplay(cm, display_labels = model.classes_ ).plot()